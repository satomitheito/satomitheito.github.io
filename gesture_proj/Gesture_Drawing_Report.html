<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Satomi Ito">

<title>Gesture Based Drawing Tool</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="Gesture_Drawing_Report_files/libs/clipboard/clipboard.min.js"></script>
<script src="Gesture_Drawing_Report_files/libs/quarto-html/quarto.js"></script>
<script src="Gesture_Drawing_Report_files/libs/quarto-html/popper.min.js"></script>
<script src="Gesture_Drawing_Report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Gesture_Drawing_Report_files/libs/quarto-html/anchor.min.js"></script>
<link href="Gesture_Drawing_Report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Gesture_Drawing_Report_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Gesture_Drawing_Report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Gesture_Drawing_Report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Gesture_Drawing_Report_files/libs/bootstrap/bootstrap-c0367b04c37547644fece4185067e4a7.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Gesture Based Drawing Tool</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Satomi Ito </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>This project implements a gesture-based drawing tool that allows users to create digital art using hand gestures captured by a webcam. The system uses computer vision and machine learning to recognize different hand gestures and translates them into drawing actions. By leveraging MediaPipe for hand tracking and TensorFlow for gesture recognition, the application enables an intuitive drawing experience without requiring traditional input devices.</p>
<p>Key features include:</p>
<ul>
<li>Real-time hand gesture recognition</li>
<li>Multiple drawing gestures (pointing to draw, flat hand to erase, etc.)</li>
<li>Brush size control using thumbs up/down gestures</li>
<li>Color selection through a color palette interface</li>
<li>Support for both overlay mode (drawing over camera feed) and whiteboard mode</li>
</ul>
</section>
<section id="data-collection" class="level1">
<h1>Data Collection</h1>
<section id="about-mediapipe" class="level2">
<h2 class="anchored" data-anchor-id="about-mediapipe">About MediaPipe</h2>
<p>MediaPipe is an open-source framework developed by Google that provides ML solutions for computer vision tasks. For this project, MediaPipe Hands is used to detect and track hand landmarks in real-time from webcam input.</p>
<p>The framework identifies 21 3D landmarks on each hand, corresponding to key points like the wrist, knuckles, and fingertips. These landmarks contain x, y, and z coordinates normalized to the image dimensions, making the system robust to various hand orientations, sizes, and lighting conditions.</p>
<p>The landmarks follow a specific indexing pattern:</p>
<ul>
<li>Index 0: Wrist</li>
<li>Indices 1-4: Thumb (from base to tip)</li>
<li>Indices 5-8: Index finger (from base to tip)</li>
<li>Indices 9-12: Middle finger (from base to tip)</li>
<li>Indices 13-16: Ring finger (from base to tip)</li>
<li>Indices 17-20: Pinky finger (from base to tip)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/hand-landmarks.png" class="img-fluid figure-img"></p>
<figcaption>hand coordinates</figcaption>
</figure>
</div>
<p>These landmark positions form the foundation for our gesture recognition system, allowing us to identify specific hand poses based on the spatial relationships between landmarks.</p>
</section>
<section id="hand_data_collection.py" class="level2">
<h2 class="anchored" data-anchor-id="hand_data_collection.py">hand_data_collection.py</h2>
<p>The <code>hand_data_collection.py</code> script creates a dataset of hand gestures for training the gesture recognition model. It captures webcam feed and processes it using MediaPipe Hands to detect hand landmarks in real-time. Users can specify a gesture class (0-6) and capture multiple samples of that gesture to build a comprehensive dataset.</p>
<p>The landmark data preprocessing involves three key steps:</p>
<ol type="1">
<li>Converting to relative coordinates (using the wrist as the base point)</li>
<li>Flattening the coordinates into a 1D array (42 values: 21 landmarks × 2 coordinates)</li>
<li>Normalizing the values to account for different hand sizes</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/how_coordinates_are_processed_for_collection.png" class="img-fluid figure-img"></p>
<figcaption>Hand Landmark Processing</figcaption>
</figure>
</div>
<p>As shown in the image above, the hand landmark data undergoes three critical transformations:</p>
<p><strong>First iteration (Raw Coordinates)</strong>: The top table shows the raw pixel coordinates captured from MediaPipe, where each landmark (ID_0 to ID_20) has specific x,y positions on the screen. These absolute coordinates depend on where the hand appears in the camera frame.</p>
<p><strong>Second iteration (Relative Coordinates)</strong>: The middle table displays how these coordinates are transformed to be relative to the wrist (ID_0), which becomes [0,0]. All other landmarks are offset from this reference point. The hand can appear anywhere in the frame and still produce similar relative measurements.</p>
<p><strong>Third iteration (Normalized Values)</strong>: The bottom table shows the final normalization step, where all values are scaled to a range between -1 and 1 by dividing by the maximum absolute value. This creates scale-invariant features that work regardless of hand size or distance from the camera.</p>
<p>These preprocessing steps are crucial for creating a robust dataset that can recognize gestures reliably across different users, hand sizes, and positions within the frame.</p>
<p>The script supports seven predefined gestures: size up (thumbs up), size down (thumbs down), nothing (neutral hand position), erase (flat hand), point (index finger extended), color (C-shaped hand), and random (custom gesture).</p>
<p>Dataset creation involves toggling between detection and collection modes, with on-screen guidance to help users capture consistent and varied samples of each gesture class.</p>
</section>
<section id="about-data" class="level2">
<h2 class="anchored" data-anchor-id="about-data">About Data</h2>
<p>For optimal model performance, multiple samples (typically 200+ per gesture) were collected for each gesture class, with variations in:</p>
<ul>
<li>Hand orientation</li>
<li>Slight differences in gesture execution</li>
<li>Lighting conditions</li>
<li>Distance from camera</li>
</ul>
<p>This diversity in training data helps the model learn robust representations of each gesture class.</p>
</section>
<section id="gestures" class="level2">
<h2 class="anchored" data-anchor-id="gestures">Gestures</h2>
<p>The system recognizes seven distinct hand gestures:</p>
<p><strong>Size Up (Thumbs Up)</strong> is used to increase brush thickness.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/sizeup.png" class="img-fluid figure-img"></p>
<figcaption>sizeup</figcaption>
</figure>
</div>
<p><strong>Size Down (Thumbs Down)</strong> decreases brush thickness.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/sizedown.png" class="img-fluid figure-img"></p>
<figcaption>sizeup</figcaption>
</figure>
</div>
<p><strong>Nothing (Neutral Hand)</strong> represents the default state with no specific action.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/nothing.png" class="img-fluid figure-img"></p>
<figcaption>sizeup</figcaption>
</figure>
</div>
<p><strong>Erase (Flat Hand)</strong> activates eraser mode.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/erase.png" class="img-fluid figure-img"></p>
<figcaption>sizeup</figcaption>
</figure>
</div>
<p><strong>Point (Index Finger)</strong> is the primary drawing gesture. The index finger is used for drawing, erasing, and selecting.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/point.png" class="img-fluid figure-img"></p>
<figcaption>sizeup</figcaption>
</figure>
</div>
<p><strong>Color (C-Shape)</strong> activates the color picker.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/color.png" class="img-fluid figure-img"></p>
<figcaption>sizeup</figcaption>
</figure>
</div>
<p><strong>Random (Custom)</strong> serves as a rejection class for gestures that are similar to the defined gestures but should not trigger any action. By including these ambiguous hand positions in training, the system reduces false positives and improves overall gesture recognition accuracy.</p>
<p>Each gesture was designed to minimize potential recognition errors while maintaining intuitive mapping to drawing operations.</p>
</section>
</section>
<section id="training" class="level1">
<h1>Training</h1>
<section id="why-cnn" class="level2">
<h2 class="anchored" data-anchor-id="why-cnn">Why CNN</h2>
<p>Convolutional Neural Networks (CNNs) were selected for this gesture recognition system due to their ability to identify spatial patterns crucial for hand gesture recognition. CNNs excel at automatically extracting relevant features from landmark data without manual engineering, while developing translation invariance that allows them to recognize gestures regardless of where the hand appears in the frame. Through convolutional and pooling operations, these networks efficiently reduce dimensionality while preserving structural information, making them computationally efficient for real-time applications.</p>
</section>
<section id="keypoint_train_1d.py" class="level2">
<h2 class="anchored" data-anchor-id="keypoint_train_1d.py">keypoint_train_1d.py</h2>
<p>The <code>keypoint_train_1d.py</code> script implements a 1D Convolutional Neural Network (CNN) for hand gesture classification. After loading the landmark data from the CSV file, it splits the data into features (X) and labels (y), then performs an 80/20 train/test split while maintaining class distribution.</p>
<p>The model architecture consists of:</p>
<ul>
<li>Input shape: (42,) representing flattened hand landmark coordinates</li>
<li>Reshaping to (42, 1) for 1D convolution operations</li>
<li>Three Conv1D blocks with increasing filter sizes (32, 64, 128)</li>
<li>Each block containing Batch Normalization and ReLU activation</li>
<li>Global Average Pooling followed by Dense layers for classification</li>
<li>Softmax output layer with 7 units (one per gesture class)</li>
</ul>
<p>The training process uses Adam optimizer with a learning rate of 0.001 and implements early stopping, model checkpointing, and learning rate reduction. The model trains for up to 100 epochs with a batch size of 32, with evaluation including visualizations of training/validation accuracy/loss and a confusion matrix to evaluate class-specific performance.</p>
</section>
<section id="keypoint_train.ipynb" class="level2">
<h2 class="anchored" data-anchor-id="keypoint_train.ipynb">keypoint_train.ipynb</h2>
<p>The <code>keypoint_train.ipynb</code> notebook implements a 2D CNN approach to gesture recognition. This method transforms the 21 hand landmarks into a 5×5 grid based on anatomical positions, converting the data from a sequence to a spatial representation.</p>
<p>The 2D CNN architecture uses these spatial relationships with three convolutional blocks processing an input shape of (5, 5, 2). This approach treats the hand as a coherent structure rather than a sequence of points, potentially capturing more complex spatial patterns.</p>
<p>Data augmentation techniques including rotations, scaling, and translations were implemented to improve model robustness. Visualizations throughout the notebook show activation patterns and reveal which hand regions contribute most to gesture classification.</p>
</section>
<section id="results-cnn1d-vs-cnn2d" class="level2">
<h2 class="anchored" data-anchor-id="results-cnn1d-vs-cnn2d">Results (CNN1D VS CNN2D)</h2>
<p>The performance comparison between 1D and 2D CNN approaches revealed distinct advantages for each method, as demonstrated in the training visualizations below:</p>
<p><strong>CNN1D Results</strong></p>
<p><img src="images/cnn1d.png" class="img-fluid"></p>
<p><strong>CNN2D Results</strong></p>
<p><img src="images/cnn2d.png" class="img-fluid"></p>
<p>For an AR drawing application requiring real-time performance on standard hardware, the 1D CNN model provided the optimal balance between accuracy and efficiency.</p>
</section>
</section>
<section id="gesture-recognition" class="level1">
<h1>Gesture Recognition</h1>
<section id="index-finger-tip-tracking" class="level2">
<h2 class="anchored" data-anchor-id="index-finger-tip-tracking">Index Finger Tip Tracking</h2>
<p>In the gesture recogniiton model, using the hand landmark coorindates, the tip of the index finger is isolated and kept track of as it is going to be the key factor in the drawing tool.</p>
</section>
<section id="gesture_recognition_1d.py" class="level2">
<h2 class="anchored" data-anchor-id="gesture_recognition_1d.py">gesture_recognition_1d.py</h2>
<p>The <code>gesture_recognition_1d.py</code> script implements real-time gesture recognition using the trained 1D CNN model. After loading the model and importing gesture class labels, it processes webcam input in real-time using MediaPipe Hands to detect landmarks, applying the same preprocessing as during training before feeding the data to the CNN model.</p>
<p>The system provides immediate visual feedback by displaying the predicted gesture and confidence score on the video feed. A particularly useful feature is the continuous learning capability, which includes a training mode for adding new samples on-the-fly. This allows incremental model updates to improve performance for specific users or environments, with updated model weights saved after each training session.</p>
</section>
<section id="gesture_recognition.py" class="level2">
<h2 class="anchored" data-anchor-id="gesture_recognition.py">gesture_recognition.py</h2>
<p>The <code>gesture_recognition.py</code> script provides an alternative implementation exploring 2D CNN-based gesture recognition. While similar to the 1D version in its core functionality, it takes a different approach to data representation and visualization.</p>
<p>Instead of treating landmarks as a 1D sequence, this version converts the data to a 2D grid representation that preserves spatial relationships between landmarks. This is complemented by enhanced visualizations including detailed confidence score displays and heatmap overlays showing landmark importance.</p>
<p>The script includes performance metrics that track recognition speed (FPS) and monitor prediction stability over time, providing insights into the real-world performance of the system. Advanced features include gesture sequence recognition for complex commands and temporal smoothing to reduce prediction jitter.</p>
</section>
</section>
<section id="putting-it-all-together" class="level1">
<h1>Putting It All Together</h1>
<section id="gesture_drawing.py" class="level2">
<h2 class="anchored" data-anchor-id="gesture_drawing.py">gesture_drawing.py</h2>
<p>The <code>gesture_drawing.py</code> script integrates gesture recognition with drawing functionality to create a complete interactive drawing application. The system recognizes specific hand gestures and maps them to different drawing operations, creating an intuitive interface that doesn’t require traditional input devices.</p>
<section id="drawing-with-index-finger" class="level3">
<h3 class="anchored" data-anchor-id="drawing-with-index-finger">Drawing with Index Finger</h3>
<p>The most basic interaction is drawing with the index finger. When the system detects the “point” gesture, it tracks the movement of the index fingertip and creates lines on the canvas.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/drawdemo.png" class="img-fluid figure-img"></p>
<figcaption>Drawing Demo</figcaption>
</figure>
</div>
</section>
<section id="eraser-mode" class="level3">
<h3 class="anchored" data-anchor-id="eraser-mode">Eraser Mode</h3>
<p>When the user makes the “erase” gesture (an open palm), the system activates eraser mode. In this mode, the index finger acts as an eraser, removing content from the canvas.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/erasedemo.png" class="img-fluid figure-img"></p>
<figcaption>Erase Mode Demo</figcaption>
</figure>
</div>
</section>
<section id="brush-size-control" class="level3">
<h3 class="anchored" data-anchor-id="brush-size-control">Brush Size Control</h3>
<p>Users can increase or decrease the brush thickness using “size up” (thumbs up) and “size down” (thumbs down) gestures.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/sizeupdemo.png" class="img-fluid figure-img"></p>
<figcaption>Size Up Demo</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/sizedowndemo.png" class="img-fluid figure-img"></p>
<figcaption>Size Down Demo</figcaption>
</figure>
</div>
<p>The system includes a brush size preview on top right that shows the current thickness selection, providing immediate visual feedback. Size changes have a built-in cooldown period to prevent accidental multiple adjustments.</p>
</section>
<section id="color-selection" class="level3">
<h3 class="anchored" data-anchor-id="color-selection">Color Selection</h3>
<p>The “color” gesture (C-shaped hand) activates the color picker interface, allowing users to select from multiple color options.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/colordemp.png" class="img-fluid figure-img"></p>
<figcaption>Color Picker Demo</figcaption>
</figure>
</div>
</section>
<section id="additional-features" class="level3">
<h3 class="anchored" data-anchor-id="additional-features">Additional Features</h3>
<p>The application supports multiple drawing modes (overlay and whiteboard), allowing users to either draw over the camera feed or on a blank canvas. File management capabilities let users save their drawings as image files and clear temporary or all drawings as needed.</p>
</section>
</section>
</section>
<section id="app-deployment" class="level1">
<h1>App Deployment</h1>
<section id="streamlit_app.py" class="level2">
<h2 class="anchored" data-anchor-id="streamlit_app.py">streamlit_app.py</h2>
<p>The <code>streamlit_app.py</code> script wraps the gesture drawing application in a web-based interface using Streamlit. This approach makes the technology more accessible to users by providing a clean, user-friendly frontend.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/streamlit.png" class="img-fluid figure-img"></p>
<figcaption>Streamlit App</figcaption>
</figure>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>